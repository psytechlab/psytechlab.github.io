--- 
layout: post 
title: Девлог 7. Генерить или не генерить? Промежуточные результаты с синтетикой для психодомена
use_math: false
--- 

В нашем домене есть одна большая проблема — данных мало и/или их трудно достать. Связано это с этикой терапии, либо врачебной тайной, если диагноз психиатрический. Генерация данных через БЯМ может смягчить боль всех ml-инженеров, но как узнать, до какой степени? Мы пытаемся ответить этот вопрос и в этом посте коротенько расскажем промежуточные результаты.

Мы взяли три датасета: сентиметы, эмоции и антисуицидальную часть нашего датасета. Взяли именно их, потому что в статьях по ним есть описание классов. Без него не составить затравку. То есть, конечно, можно, но нам хотелось, чтобы затравка была связана с инструкцией по созданию датасета, мост навести, так сказать.

Про затравку еще кое-что. Мы тестировали три варианта: zero-shot, few(8)-shot и few-shot with keywords (fskw). Готовы поспорить, вы никогда не слышали про третий. Потому что это наша задумка. Идея вот в чем. Как бы вы не старались, но вы не сможете в какое-то резонное количество примеров для режима few-shot запихнуть различные лексические особенности класса. Тем более, что в некоторых работах отмечается, что с ростом примеров начинается деградация качества. Вот мы и решили добавлять такие классово-значимые слова в затравку.

Еще у нас было шесть моделей, три закрытых и три открытых, и два значения температуры: 0.7 и 1.

С помощью каждой комбинации этих параметров, мы сгенерировали для каждого класса каждого датасета уникальных примеров не меньше, чем в оригинальном датасете. Уже тут есть что сказать: некоторые конфиги генерили столько, сколько нужно, а другие генерировали до 15к текстов, Карл!, прежде, чем получить нужных пару тысяч уникальных среди них. Как только мы всё сгенерили, начали по-разному смешивать их с разными данными и обучать классификаторы.

Мы начали с простого: обучить только на генерате, на половине реального и половине генерата, на всем реальном и таком же количестве генерата. Второй и третий вариант мы рассматривать не будем в этом посте (tl;dr: результаты плюс-минус такие же, как в исходном варианте, где все данные реальны), обойдемся первым.

Если смотреть на качество итогового классификатора по f1-macro в разрезе каждой части конфигурации, то в среднем лучшей моделью стала llama-4-maveric (помните такую?) среди всех трех сеттингов. Изменение температуры влияло на качество только на антисуицидальном датасете: значение 0.7 в среднем лучше на 4 пункта, чем значение 1.0. Наша придумка fskw также показала лучшие результаты по всем трем датасетам.

Нам известно из второго пополамчатого сеттинга, что качество классифакции очень близко к исходному, где все данные реальные. А какого качества мы можем добиться, если будем добавлять генерат в минимальное количество реальных данных? 

Для начала мы нашли этот условный минимум. Для всех трех датасетов это оказалось 10 процентов от реального объема. Потом в эти 10 процентов добавляли генерат объемом в 25, 50, 75 и 100 процентов от исходного объема каждого класса. Получилось что уже в первом случае для двух из трех датасетов происходит резкий рост качества модели (в целом, такое же рост наблюдается между 10 и 20 процентами реальных данных). После взлета наблюдается уже не такой заметный, но тоже прирост качества. Лучшие варианты миксов отстают от исходной модели на 3-5 пунктов. Кстати, далеко не факт, что лучшим миксом окажется вариант со 100 процентами генерата. Датасет сентиментов стал тем, где этот фокус не получился.

Далее у нас ряд экспериментов с разными метриками данных. Наша цель — найти метрику, которая по паре тысяч семлов модели могла сказать, что использовать перспективно, а что не очень.
